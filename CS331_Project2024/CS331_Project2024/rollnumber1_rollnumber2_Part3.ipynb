{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 3A: Neural Network [40 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement a neural network and test its path prediction performance on the same dataset using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "# make other necessary imports here\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Dataset\n",
    "\n",
    "For this task, you will need to consider three different scenarios so you will need to process data three times.\n",
    "- Predict the movement of the robot for the next 3 seconds i.e., 90 datapoints (x, y) at once based on previous:\n",
    "\n",
    "    * 3 seconds i.e., 90 previous datapoints.\n",
    "    * 6 seconds i.e., 180 previous datapoints.\n",
    "    * 9 seconds i.e., 270 previous datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "def create_lookback_dataset(data, lookback):\n",
    "   \n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:(i + lookback)].flatten())  # Flattening the window\n",
    "        Y.append(data[i + lookback])\n",
    "    return np.array(X), np.array(Y) \n",
    "def process_data(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    for line in lines:          # process the lines to extract x and y coordinates\n",
    "        x_str, y_str = line.strip().split(',')\n",
    "        x = int(x_str)\n",
    "        y = int(y_str)\n",
    "        data.append([x, y])     # store x and y coordinates for each time step\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "train = process_data(r'Dataset/Training/training_data.txt')\n",
    "test = process_data(r'Dataset/Testing/test01.txt')\n",
    "look_back = [90,180,270]\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in look_back:\n",
    "    X, Y = create_lookback_dataset(train, i)\n",
    "    x_train.append(X)\n",
    "    y_train.append(Y)\n",
    "    X, Y = create_lookback_dataset(test, i)\n",
    "    x_test.append(X)\n",
    "    y_test.append(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use [Adam](https://pytorch.org/docs/stable/optim.html) optimizer and RMSE function from Part 1A for calculating neural network loss during the training of model. Visit the embedded link to know more about the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mNeuralNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)   \u001b[38;5;66;03m# feel free to tune this hyperparameter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m    \u001b[38;5;66;03m# feel free to tune this hyperparameter\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# code here\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(NeuralNet.parameters(), lr=0.01)   # feel free to tune this hyperparameter\n",
    "epochs = 100    # feel free to tune this hyperparameter\n",
    "\n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Analysis\n",
    "\n",
    "*  Plot a line graph to evaluate your model's performance (using code for RMSE from Part 1A) across the lookback size range `i.e., 90 (3 sec), 180 (6 sec), 270 (9 sec)`. Identify and explain any trend in how the RMSE values change with varying lookback size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click to $\\color{green}{\\text{add explanation/reasoning here}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Actual and Predicted Path\n",
    "\n",
    "* Modify and use same code from the previous parts. The time interval should be 6 seconds (choose from wherever in the testing data). Note that this time your model predicts 3 seconds. Think about how to deal with this.\n",
    "* Generate a graph illustrating the actual and predicted paths using one of the above lookback sizes. Which one should you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "* How does changing the number of layers and neurons affect the error? Try out 3-4 reasonable combinations of these hyperparameters (i.e. evenly spaced enough to show some appreciable difference etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click to $\\color{green}{\\text{add explanation/reasoning here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 3B (Bonus Part): Mapping the Predicted Path onto the Video [5 marks]**\n",
    "\n",
    "* Select the best-performing NN model from the ones you implemented above. Overlay the actual and predicted paths on the video frame. Sample is provided below.\n",
    "* OpenCV documentation might be helpful for this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"620\" height=\"440\" controls>\n",
    "  <source src=\"./bonus_sample.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
